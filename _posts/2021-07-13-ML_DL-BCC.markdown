---
layout: post
title:  "[ML Paper] Bayesian Co-clustering"
subtitle:   "Recommendation"
categories: ml_dl
tags: ml_paper
comments: true
---

|![Title](https://swha0105.github.io/assets/ml/img/BCC_title.png)  
|:--:| 
| [논문 링크](https://arindam.cs.illinois.edu/papers/08/bcc.pdf) |  

<br/>

---

# 선행지식
이 논문을 이해하러면 `Co-clustering`에 대한 이해가 필수지만 자세한 설명을 찾기 힘들어 [본 강의](https://www.youtube.com/watch?v=mnDC6hWWbwY)를 참조 하였다. 개인적으로 느끼기에 co-clustering에 대해 가장 잘 설명한듯 하다.


|![CC sample](https://swha0105.github.io/assets/ml/img/cc_fig1.png)  
|:--:| 
| Example data |  

- 동물과 그 특징을 나타내는 dataset
- 비슷한 특징들을 나타내는 동물들을 잘 묶어내는게 goal

## PCA

|![PCA example](https://swha0105.github.io/assets/ml/img/cc_fig2.png)  
|:--:| 
| PCA example |  

- feature들이 많을때 흔히 가장 많이 사용하는 PCA기법.
- PCA axis 1, component 1은 birds와 아닌것들의 구분.
- PCA axis 2, component 2는 사실상 구분이 불가능.

### PCA의 문제점
- Not interest in how every sample affected by every variable (vice versa)
(예를 들어, 개한테 털이 얼마나 달렷는지 관심없음)

- 따라서 PCA와는 다르게 Co-clustering은 group of data는 group of feature(variable)로 표현 하도록 설정 

<br/>

## Co-clustering

|![Co-clustering example](https://swha0105.github.io/assets/ml/img/cc_fig3.png)  
|:--:| 
| Clustering example, top right: Traditional clustering, bottom right: Co-clustering |  

- Row index는 사람이름, Column index는 제출한 논문 저널을 의미.
- 이때, 논문저널 제출 패턴이 비슷한 사람끼리 묶어본다. 

- traditional clustering은 row끼리 가장 비슷한 묶는다. (Rasmus Bro - PAT person)
- co-clustering은 row와 column모두 고려하여 가장 비슷한 데이터끼리 묶는다.   
(EEM은 Rasmus Bro, Fluorescence person. Rasmus Bro의 NIR,PLS,PCA는 PAT person과 clustering)

**Co-clustering 핵심: some of the samples are reflected by some of variables**


<br/>

---


# 1. Abstact & Introduction


- 많은 실제데이터들이 2가지 `entities of interest` (row & column in matrix)에 영향을 받는 `dyadic`의 형태를 띄고있다.  

- dyadic data와 관련된 data mining의 목적은 각 entity를 clustering하는 것이다.
(ex, movie and user groups in recommendations system)

- 하지만 traditional clustering algorithm은 2가지 entitiy의 연관성을 캡쳐할 수 없어 이러한 문제에 제대로된 성능을 내지 못하였다.

- `co-clustering`은 2가지 entity에 대해 동시에 clustering을 수행함으로서, data의 structure를 파악하고 missing value의 값을 예측하는데 강점이 있다.

- `co-clustering`의 주요 제한은 **`partional`**이다. (row/column이 단 하나의 row/column cluster에 속함) 이러한 제한은 데이터가 multiple cluster에 속하는 실제데이터를 표현하는데 한계가 있다. (user might be an action movie fan and also a cartoon movie fan)

- 따라서, 이 문제를 해결하기 위해 본 논문에 제시하는 **`Bayesian co-clustering(BCC)`**은 각 row/column은 row/column cluster에서 생성된 mixed membership을 가지고 있다고 가정한다.

- BCC는 row/column co-cluster를 생성하는 generative model로서 any exponential family distribution을 사용할 수 있다. 

- BCC는 non missing entries를 기반으로 inference를 시행하기에 sparse matrices를 처리하는데 특화되어있음.

- Model은 각 row/column에 대해 동시에 dimensional reduction을 하는 `Co-embedding`을 수행한다

<br/>

---

# 2. Generative Mixture Models
- Background for Generative model in BCC

## **2.1 Finite Mixture Model**

$$ p(X \lvert \pi, \Theta) = \sum_{z=1}^{k}p(z \lvert \pi)p(X \lvert \theta_{z})$$

> $$\pi$$: K component distribution의 prior  
> $$\Theta: (\theta_{z},z_{1}^{k})$$   
> $$z_{1}^{k}: z \quad where \; z \in (1,2,....k)$$  
> $$p(x \lvert \theta_{z})$$: exponential family distribution

- latent cluster structure를 찾기 위한 가장 많이 연구된 모델
- prior $$\pi$$가 all data point에서 고정되어있다고 가정.


## **2.2 Latent Dirichlet Allocation**

$$P(X \lvert \alpha, \Theta) = \int_{\pi} Dir(\pi \lvert \alpha) (\Pi_{l=1}^{d} \sum_{z_{1}}^{k} p(z_{l} \lvert \pi)p(x_{l} \lvert \theta_{z_{l}})  ) d \pi $$

- Finite Mixture Model에서 가정한 Fixed $$\pi$$를 해결하기 위해 $$\pi$$를 Dirichlet distribution($$Dir(\alpha)$$)에서 sampled 된다고 가정하고 $$\pi$$를 mixing weight라고 부른다.
- $$P(X \lvert \alpha, \Theta)$$는 intractable이기 때문에 Variational Inferecnce와 Gibbs sampling이 도입
- token (X = (x1,x2,x3...))은 discrete임을 가정한다.

## **2.3 Bayeisan Baive Bayes.**

$$P(X \lvert \alpha, \Theta, F) = \int_{\pi} Dir(\pi \lvert \alpha) (\Pi_{l=1}^{d} \sum_{z_{1}}^{k} p(z_{l} \lvert \pi)p_{\psi}(x_{l} \lvert \theta_{z_{l}},f_{l},\Theta )  ) d \pi $$

> F: feature set  
> $$f_{l}:$$: feature for l-th non-missing entry 

- Latent Dirichlet Allocation에서 가정한 discrete token을 해결하기 위한 방법
- 자세한 설명은 기술되어있지 않다. 다른 논문을 찾아 조사가 더 필요할듯 하다

## **2.4 Co-clustering based on GMMs**
- GMM을 기반으로 한 Co-clustering을 하여 data mining에 적용한 기존 연구들은 다음과 같은 단점을 지닌다.
  1. Binary relationship만 다룰 수 있다
  2. one type of entity만 다룰 수 있다. 
  3. 효과적인 inference algorithm이 존재하지 않는다.

- 여기서 제시하는 BCC는 위와 같은 단점이 존재하지 않는다고 한다. 

<br/>

---

# 3. Bayesian Co-Clustering

|![BCC model](https://swha0105.github.io/assets/ml/img/BCC_fig1.png)  
|:--:| 
| Bayesian Co-Clustering Model |  

## **3.1 Notation & Assumption**
- Data Matrix: X (n1 * n2 )
- row cluster k1에 대한 latent variable:  $$z1 = (i,{i}_{1}^{k_{1}}$$) 
- column cluster k2에 대한 latent variable:  $$z2 = (j,{j}_{1}^{k_{2}}$$)
> ($$ i,{i}_{1}^{k_{1}} $$) 은 $$i$$값이 $$1,2,3, .. k_{1}$$을 가질수 있다는 의미 
~~왜 이런 notation을 사용하는지는 의문이다..~~

- rows/columns은 $$Dir(\alpha_{1})$$/$$Dir(\alpha_{2})$$ 분포를 따른다.
- Dirichlet 분포(Dir) 에서 각 row/column (u/v)에 대해 해당되는 **mixing weight ($$\pi_{1u}$$  $$\pi_{2v}$$)**가 만들어진다.
  - Dirchlet분포에서 각 row/column에 해당되는 분포 생성
- row/column (u/v)에 해당되는 **row/column clusters**는 discrete distributiuon ($$Disc(\pi_{1u}) / Disc(\pi_{2v})$$)에서 sampled된다.
  - mixing weight에서 row/column cluster 생성
- **row/column cluster (i/j)는 Co-cluster (i,j)에 해당된다.**  
  Co-cluster는 exponential family distribution $$p_{psi}(x \lvert \theta_{ij})$$  $$\theta_{ij}$$에 해당된다.
  - row/column cluster 분포에서 co-cluster 생성


## **3.2 Generative Process**

1. $$Dir(\alpha_{1})/Dir(\alpha_{2})$$에서 Matrix에 있는 모든 row/column 에 대해 mixing weight $$\pi_{1u}/\pi_{2v}$$을 샘플링한다.
2. Mixing weight를 input으로 받는 Discrete distribution에서 $$(Disc(\pi_{1u})/Disc(\pi_{2v}))$$
에서 row/column cluster의 latent variable $$(z_{1}/z_{2})$$를 샘플링한다.
3. 추출된 $$(z_{1},z_{2})$$을 이용하여 data x를 생성한다 ( $$p(x \lvert \theta_{z1,z2})$$ )

<!-- ※ $$z_{1},z_{2}$$에 대한 설명은 논문에 명시되어있지 않다. 본인이 논문 내용을 추론하여 작성한것으로 정확한내용과 다를 수 있다.  -->

제시된 model을 이용하여 **하나의 entry (matrix element x)**에 대한 **marginal probability**를 계산하면 다음과 같다.

$$ p(x \lvert \alpha_{1},\alpha_{2},\Theta) = \int_{\pi_{1}}\int_{\pi_{2}} p(\pi_{1} \lvert \alpha_{1})p(\pi_{2} \lvert \alpha_{2}) \sum_{z1} \sum_{z2} p(z_{1} \lvert \pi_{1}) p(z_{2} \lvert \pi_{2}) p(x \lvert \theta_{z_{1}z_{2}}) d \pi_{1} d \pi_{2}$$

이 식을 따르면, row/column에 해당되는 mixing weight는 ($$\pi_{1} / \pi_{2}$$) 한번씩만 샘플링되기 때문에, 각 row/column에 안에 속해있는 entry(data) 끼리는 coupling이 일어 난다.  
(Not statistically independent, 하나의 row/column에 속해있는 entry(data)들은 $$\pi_{1},\pi_{2}$$ 값이 공유됨)   
따라서, **전체 matrix의 확률을 구하기 위해 하나의 entry에 대해 marginal probability구하는 접근은 불가능 하다**

따라서, **전체 matrix의 joint probability**를 한번에 구하기 위해 각 point (entry)의 marginal probability의 곱과 같다고 가정하고 식을 풀면 다음과 같다. 이 가정은 mixture model에서 많이 사용하는 가정이다.


$$ p(X, \pi_{1u}, \pi_{2v}, z_{1uv},z_{2uv},u_{1}^{n1},v_{1}^{n2} \lvert \alpha_{1},\alpha_{2},\Theta) = $$  
 $$ ( \Pi_{u} p(\pi_{1u} \lvert \alpha_{1}) ) ( \Pi_{v} p(\pi_{2v} \lvert \alpha_{2}) )
( \Pi_{u,v} p(z_{1uv} \lvert \pi_{1u}) p(z_{2uv} \lvert \pi_{2v}) p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}} )^{\delta_{uv}}    )$$

> $$\delta_{uv}$$: if $$x_{uv}$$ is missing then 0 otherise 1  (non-missing entry만 고려한다)  
> $$z_{1uv}$$: latent row cluster $$(z_{1uv} \in {1,2,... k_{1}})$$ for observation $$x_{uv}$$  
> $$z_{2uv}$$: latent column cluster $$(z_{2uv} \in {1,2,... k_{2}})$$ for observation $$x_{uv}$$  

- observation $$x_{uv}$$ (entry, data point)은 $$\pi_{1u}, u_{1}^{n_{1}}$$과 $$\pi_{2v}, v_{1}^{n_{2}}$$가 주어졌을때 **`Conditionally Indepenent`** 하다. 이 조건을 고려하고 **joint distribution**을 다시 쓰게 되면 아래와 같다.

$$ p(X, \pi_{1u}, \pi_{2v}, z_{1uv},z_{2uv},u_{1}^{n1},v_{1}^{n2} \lvert \alpha_{1},\alpha_{2},\Theta) = $$  
 $$ ( \Pi_{u} p(\pi_{1u} \lvert \alpha_{1}) ) ( \Pi_{v} p(\pi_{2v} \lvert \alpha_{2}) )
( \Pi_{u,v} p(z_{1uv} \lvert \pi_{1u}) p(z_{2uv} \lvert \pi_{2v}) p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}} )^{\delta_{uv}}    )$$

> marginal probability: $$ p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}} ) = \sum_{z_{1uv}} \sum_{z_{2uv}} p(z_{1uv} \lvert \pi_{1u}) p(z_{2uv} \lvert \pi_{2v}) p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}})$$


- 위의 식을 $$\pi_{1u},\pi_{2v}$$에 대해 marginlizing하여 Matrix X에 대해 확률을 다시 쓰게 되면 다음과 같다.

$$ p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) = $$  
$$ \int \int (\Pi_{u} p(\pi_{1u} \lvert \alpha_{1}) ) (\Pi_{v} p(\pi_{2v} \lvert \alpha_{2}) )
\Pi_{u,v} \sum_{z_{1uv}} \sum_{z_{2uv}} $$  
$$p(z_{1uv} \lvert \pi_{1u}) p(z_{2uv} \lvert \pi_{2v}) p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}})^{\delta_{uv}} d \pi_{11} d \pi_{12} ... d \pi_{1n_{1}} d \pi_{21} d \pi_{22} ... d \pi_{2n_{2}} $$

<br/>

---


# 4. Inference and Learning

- BCC model의 learning task는 을 likelihood of observing matrix X을 최대화 하는 $$\alpha_{1}, \alpha_{2}, \Theta$$ 값을 추론 하는 것이다.

- $$ p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) $$ 은 Intractable 하기에 이 함수의 log-likelihood의 lower bound 함수를 정의한다.

- lower bound를 최대화 하는 model parameter ($$\alpha_{1}, \alpha_{2}, \Theta$$)를 찾는다
- lower bound를 찾기 위해, entire family of parameterized lower bounds와 이에 대한 set of free variational parameters를 고려할 것이다.


## **4.1 Variational Approximation**
- [Variational Approximation 정리한 글](https://swha0105.github.io/ml_dl/2021/07/13/ML_DL-varience_inference/)
- $$ p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) $$의 lower bound를 찾기 위해 **latent variable distribution을 ($$p(z_{1},z_{2},\pi_{1},\pi_{2} \lvert \alpha_{1}, \alpha_{2}, \Theta)$$)** 근사하는 아래와 같은 함수(q) 를 도입한다. 



|![BCC model](https://swha0105.github.io/assets/ml/img/BCC_fig2.png)  
|:--:| 
| Variational distribution q |  


$$q(z_{1},z_{2},\pi_{1},\pi_{2} \lvert \gamma_{1}, \gamma_{2}, \phi_{1},\phi_{2}) = $$   
$$\Pi_{u=1}^{n_{1}} q(\pi_{1u} \lvert \gamma_{1u}) \Pi_{v=1}^{n_{2}} q(\pi_{2v} \lvert \gamma_{2v}) \Pi_{u=1}^{n_{1}} \Pi_{v=1}^{n_{2}} q(z_{1uv} \lvert \phi_{1u})  q(z_{2uv} \lvert \phi_{2v}) $$
 
> $$\gamma_{1} = (\gamma_{1u}, u_{1}^{n_{1}} )$$: Dirichlet distribution parameter  
> $$\phi_{1} = (\phi_{1u}, u_{1}^{n_{1}})$$: Discrete distribution parameter  
> $$m_{u}$$ (in figure) = number of non-missing entries in row u  

- 기존 LDA나 BNB에서 쓰는 variational approximation의 방법은 다음과 같다. 각 entry의 할당된 cluster assignment(z)는 각기 다른 variational discrete distribution ($$\phi$$) 생성한다. 
- BCC는 이에 다르게 하나의 row/column에 대해 같은 variational discrete distribution을 사용한다.

- 이와 같은 방법을 통해 다음과 같은 2가지의 장점을 얻을 수 있다.
  1. row/column 안에 있는 entries들에 대해 dependency를 유지할 수 있다. 
  2. number of variational parameter가 줄기 때문에 inference가 빨라 진다.

- 전체 Matrix probability 에 대한 식은 다음과 같다.

$$ \log p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) \geq E_{q} (\log p(X,z_{1},z_{2},\pi_{1},\pi_{2} \lvert \alpha_{1}, \alpha_{2}, \Theta))$$   
$$- E_{q} (\log q(X,z_{1},z_{2},\pi_{1},\pi_{2} \lvert \gamma_{1}, \gamma_{2}, \phi_{1},\phi_{2})) $$

- 이 식의 `Lower bound`다음과 같지만 이것을 유도하거나 정리하는건 복잡하고 의미 없는거같아 논문에서도 언급만 하고 넘어 간듯하다. 

$$L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}; \alpha_{1}, \alpha_{2},\Theta) $$  
$$ = E_{q}( \log p(\pi_{1} \lvert \alpha_{1})) + E_{q}( \log p(\pi_{2} \lvert \alpha_{2})) + E_{q}( \log p(z_{1} \lvert \pi_{1})) + E_{q}( \log p(z_{2} \lvert \pi_{2})) $$
$$ + E_{q}( \log p(X \lvert z_{1}, z_{2},\Theta)) - E_{q}( \log p(\pi_{1} \lvert \gamma_{1})) -  E_{q}( \log p(\pi_{2} \lvert \gamma_{2})) - E_{q}( \log p(z_{1} \lvert \phi_{1})) -  E_{q}( \log p(z_{2} \lvert \phi_{2})) $$



### **4.1.1 Inference**

- Inference step에서는 $$ \log p(X \lvert \alpha_{1},\alpha_{2},\Theta)$$의 **lower bound (L)의 parameter $$(\alpha_{1},\alpha_{2},\Theta)$$를 설정하여 Maximize한다.**

- Lower bound의 closed form solution이 없기 때문에 Lower bound의 maximum을 구하기 위해 $$ \frac{\partial L}{\partial \phi} = 0, \; \frac{\partial L}{\partial \gamma} = 0$$ 인 $$\phi, \gamma$$을 구한다.

$$ \phi_{1ui} \propto \exp(\Psi_{1ui}) + \frac{\sum_{v,j}} \delta_{uv} \phi_{2vj} \log p(x_{uv} \lvert \theta_{ij}){m_{u}}$$  
$$\gamma_{ui} = \alpha_{1i} + m_{u}\phi_{1ui}$$

> $$\phi_{1ui}$$: row u에 대한 $$\phi_{1}$$의 i번째 component  
> $$\phi_{2vj}$$: column v에 대한 $$\phi_{2}$$의 j번째 component  
> $$\Psi$$: digamma function
> i,j: co-cluster의 index  


- $$\phi_{1ui}$$는 row u 가 cluster i 에 대해 속할 수 있을 정도  
(degree of row u belonging to cluster i)

- $$\phi$$와 $$\gamma$$를 추론하기 위해 [Simulated Annealing](https://youtu.be/qK46ET1xk2A?t=1785)을 사용하였다. 논문에 자세한 설명이 나오지 않아 링크를 참조하였다.


### **4.1.2 Parameter Estimation**

- Inference에서 $$\alpha_{1},\alpha_{2},\Theta$$을 이용하여 최적의 **($$\gamma_{1}^{*}, \gamma_{2}^{*}, \phi_{1}^{*}, \phi_{2}^{*}$$)** 을 구하였다. $$L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}; \alpha_{1}, \alpha_{2},\Theta) $$ 

- 최적의 $$\alpha$$을 구하기 위해 LDA, BNB에서 사용한 `Newton method`를 사용한다. 

$$ \alpha_{1}^{'} = \alpha_{1} + \eta H(\alpha_{1})^{-1} g(\alpha_{1})$$

- $$\alpha$$가 feasible region ($$\alpha$$ >0)에서 벗어나는걸 방지하기위해 $$\eta$$값을 조절하는 `adaptive line search`를 적용.

- $$\Theta = (\mu_{ij}, \sigma_{ij}^{2},[i]_{1}^{k1},[j]_{1}^{k2})$$는 모든 exponential family distribution의 solution이 될 수 있지만 여기서는 univariate Gaussians이라 가정한다.

$$u_{ij} = \frac{ \sum_{u=1}^{n_{1}} \sum_{v=1}^{n_{2}} \delta_{uv}x_{uv}\phi_{1ui}\phi_{2vj} }{\sum_{u=1}^{n_{1}} \sum_{v=1}^{n_{2}} \delta_{uv}\phi_{1ui}\phi_{2vj}}$$


### **4.2 EM algorithm**

- **지금까지 `intractable posterior probability` ($$\log p(X \lvert \alpha_{1},\alpha_{2},\Theta)$$)을 추론하기 위해 `Variational Approximation` ($$q(z_{1},z_{2},\pi_{1},\pi_{2} \lvert \gamma_{1},\gamma_{2},\phi_{1},\phi_{2})$$을 구성한 뒤 posterior와 차이를 최소화 하는 `Lower bound`($$L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2};\alpha_{1},\alpha_{2},\Theta)$$)을 구성했다. 이 후, `Inference`와 simulated annealing을 통해 Lower bound를 최대화 하는 variational parameters ($$\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}$$)을 계산하고 `Parameter Estimation`와 newton method 통해 최적의 Dirichlet parameters ($$\alpha_{1},\alpha_{2}$$)을 계산했다.**

**1. E-step:** given ($$\alpha_{1}^{t-1},\alpha_{2}^{t-1},\Theta^{t-1}$$)  
- $$\gamma_{1}^{t},\gamma_{2}^{t},\phi_{1}^{t},\phi_{2}^{t} = argmax_{\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}} L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2};\alpha_{1}^{t-1},\alpha_{2}^{t-1},\Theta^{t-1})$$

- 이 과정을 통해 $$\log p(X \lvert \alpha_{1},\alpha_{2},\Theta)$$의 Lower bound function (L)을 구할 수 있다.

**2. M-step:** 

- $$(\alpha_{1}^{t},\alpha_{2}^{t},\Theta^{t}) = argmax_{\alpha_{1},\alpha_{2},\Theta} L(\gamma_{1}^{t},\gamma_{2}^{t},\phi_{1}^{t},\phi_{2}^{t};\alpha_{1},\alpha_{2},\Theta)$$

- 이 과정을 통해 현재 (t)의 lower bound의 최적의 dirichlet parameter를 구할 수 있다. 


<br/>

----

# 5. Experiments

- Simulated data와 Real data(스킵)에 대해 테스트 하였다.

**5.1 Simulated data**

- 20x20의 Co-cluster(subcluster, 4 row, 5 column)으로 구성된 3개의 Data matrix (80x100)을 이용한다. 각 data matrix에 대해 generative model은 Gaussian, Bernoulli, Possion이다.

- co-cluster의 data는 generative model with pre-defined parameter 에서 생성되고 각 co-cluster의 paramter는 각기 다르다.

- 생성된 data들은 row, column단위로 randomly permute된다. permute된 데이터를 이용해 permute되기 전 데이터에 대한 정보를 추론한다(label).

- 각각의 Co-cluster의 대해 5%의 label을 주어 **semi-supervised learning**을 한다.

- Cluster accuracy(CA)(= $$\frac{1}{n} \sum_{i=1}^{k} n c_{i} $$)를 계산한다
> n: number of rows/columns  
> k: number of row/column clusters  
> $$c_{i}$$: i번재 co-cluster에 대하여, 가장 많은 수의 row/column의 parameter들이 same true cluster의 parameter와 일치하는지 알려주는 변수. ~~논문에 설명이 부족하여 추론한 내용, 틀릴 수 있음~~


|![parameter estimation](https://swha0105.github.io/assets/ml/img/BCC_fig3.png)
|:--:| 
| Top: parameter estimation for Gaussian, Top left: True, Top Right: Estimated |  
| Bottom: cluster accuracy |  

<!-- |![Cluster accuracy](https://swha0105.github.io/assets/ml/img/BCC_table.png) 
|:--:| 
| cluster accuracy |   -->


- Parameter estimation에 대해, 각 co-cluster에 대해 highest log-likelihood를 표현하는 parameter를 pick했다.
- **BCC는 generative model을 다르게하여 다양한 data type에 적용할 수 있다.**

<br/>

---

# 6. Conclusion

- BNN은 Co-clustering을 generative mixture modeling problem의 시각으로 풀어냈다.
- sparse matriex에 강하며 generative model로 any exponential family을 사용할 수 있다.
- row/column에 대해 co-cluster을 하나만 배정하는 partitional co-clustering과는 다르게 rows and column에 대해 mixed membership을 생성한다.
- 제시한 variational approximation은 stochastic approximation에 비해 significantly faster


--

<script>
MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
