---
layout: post
title:  "[ML Paper] Bayesian Co-clustering"
subtitle:   "Recommendation"
categories: ml_dl
tags: ml_paper
comments: False
---

|![Title](https://swha0105.github.io/assets/ml/img/BCC_title.png)  
|:--:| 
| [논문 링크](https://arindam.cs.illinois.edu/papers/08/bcc.pdf) |  

<br/>

---

# 선행지식
이 논문을 이해하러면 `Co-clustering`에 대한 이해가 필수다. [참조 1](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=dic1224&logNo=220144047466), [참조 2](https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Biclust) 이 페이지들을 참조하였다. 

<br/>


---


# 1. Abstact & Introduction


|![ref](https://swha0105.github.io/assets/ml/img/BCC_ref1.png)  
|:--:| 
| Biclustering 개요 [출처](https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Biclust) |  

- 많은 실제데이터들이 2가지 `entities of interest` (row & column in matrix)에 영향을 받는 `dyadic`의 형태를 띄고있다.  

- 하지만 traditional clustering algorithm은 2가지 entities의 연관성을 캡쳐할 수 없어 이러한 문제에 제대로된 성능을 내지 못하였다. (row cluster, column cluster 각각 따로)

- `co-clustering`은 2가지 entity에 대해 동시에 clustering을 수행함으로서, data의 structure를 파악하고 missing value의 값을 예측하는데 강점이 있다.

- `co-clustering`의 주요 제한은 **`partional`**이다. (row/column이 단 하나의 row/column cluster에 속함) 이러한 제한은 데이터가 multiple cluster에 속하는 실제데이터를 표현하는데 한계가 있다. (user might be an action movie fan and also a cartoon movie fan)

- 따라서, 이 문제를 해결하기 위해 본 논문에 제시하는 **`Bayesian co-clustering(BCC)`**은 다음과 같은 성질을 가진다
  - rows, columns의 사전분포에 대해 Dirichlet distribution을 가진다.
  - row/column co-cluster를 생성하는 generative model로서 any exponential family distribution을 사용할 수 있다. 이를 통해 다양한 데이터 타입에 적용이 가능하다. 

<br/>

---

# 2. Generative Mixture Models
- Background for Generative model in BCC

## **2.1 Finite Mixture Model**

$$ p(X \lvert \pi, \Theta) = \sum_{z=1}^{k}p(z \lvert \pi)p(X \lvert \theta_{z})$$

> $$\pi$$: K component distribution의 prior  
> $$\Theta: (\theta_{z},z_{1}^{k})$$   
> $$z_{1}^{k}: z \quad where \; z \in (1,2,....k)$$  
> $$p(x \lvert \theta_{z})$$: exponential family distribution

- latent cluster structure를 찾기 위한 가장 많이 연구된 모델
- prior $$\pi$$가 all data point에서 고정되어있다고 가정.


## **2.2 Latent Dirichlet Allocation**

$$P(X \lvert \alpha, \Theta) = \int_{\pi} Dir(\pi \lvert \alpha) (\Pi_{l=1}^{d} \sum_{z_{1}}^{k} p(z_{l} \lvert \pi)p(x_{l} \lvert \theta_{z_{l}})  ) d \pi $$

- Finite Mixture Model에서 가정한 Fixed $$\pi$$를 해결하기 위해 $$\pi$$를 Dirichlet distribution($$Dir(\alpha)$$)에서 sampled 된다고 가정하고 $$\pi$$를 mixing weight라고 부른다.
- $$P(X \lvert \alpha, \Theta)$$는 intractable이기 때문에 Variational Inferecnce와 Gibbs sampling이 도입
- token (X = (x1,x2,x3...))은 discrete임을 가정한다.

## **2.3 Bayeisan Baive Bayes.**

$$P(X \lvert \alpha, \Theta, F) = \int_{\pi} Dir(\pi \lvert \alpha) (\Pi_{l=1}^{d} \sum_{z_{1}}^{k} p(z_{l} \lvert \pi)p_{\psi}(x_{l} \lvert \theta_{z_{l}},f_{l},\Theta )  ) d \pi $$

> F: feature set  
> $$f_{l}:$$: feature for l-th non-missing entry 

- Latent Dirichlet Allocation에서 가정한 discrete token을 해결하기 위한 방법
- 자세한 설명은 기술되어있지 않다. 다른 논문을 찾아 조사가 더 필요할듯 하다

## **2.4 Co-clustering based on GMMs**
- GMM을 기반으로 한 Co-clustering을 하여 data mining에 적용한 기존 연구들은 다음과 같은 단점을 지닌다.
  1. Binary relationship만 다룰 수 있다
  2. one type of entity만 다룰 수 있다. 
  3. 효과적인 inference algorithm이 존재하지 않는다.

- 여기서 제시하는 BCC는 위와 같은 단점이 존재하지 않는다고 한다. 

<br/>

---

# 3. Bayesian Co-Clustering

|![BCC model](https://swha0105.github.io/assets/ml/img/BCC_fig1.png)  
|:--:| 
| Bayesian Co-Clustering Model |  


- Data Matrix: X (n1 * n2 )
- Row / Column Cluster 갯수: k1 / k2
- rows/columns은 각각 $$Dir(\alpha_{1})$$/$$Dir(\alpha_{2})$$ 분포를 따른다.


**1.** Dirichlet 분포(Dir) 에서 전체 row/column(u/v)에 대해 해당되는 **mixing weight ($$\pi_{1u}$$  $$\pi_{2v}$$)**가 만들어진다.

**2.** row/column(u/v)에 속해 있는 데이터(Entry)에 해당되는 **row / column clusters** (z1 / z2)는 discrete distributiuon ($$Disc(\pi_{1u}) / Disc(\pi_{2v})$$)에서 sampled된다.

> row cluster의 latent variable:  $$z1 = (i,{i}_{1}^{k_{1}}$$)   
> column cluster의 latent variable:  $$z2 = (j,{j}_{1}^{k_{2}}$$)  
> ($$ i,{i}_{1}^{k_{1}} $$) 은 $$i$$값이 $$1,2,3, .. k_{1}$$을 가질수 있다는 의미  

 
**3.** 결정된 row / column cluster (i/j)는 Co-cluster (i,j)을 결정하는데 사용된다. 이때, Co-cluster는 exponential family distribution을 가진다.
  
<br/>

위의 Process를 따라 **하나의 entry (matrix element x)**에 대한 **marginal probability**를 계산하면 다음과 같다.

$$ p(x \lvert \alpha_{1},\alpha_{2},\Theta) = $$
$$\int_{\pi_{1}}\int_{\pi_{2}} p(\pi_{1} \lvert \alpha_{1})p(\pi_{2} \lvert \alpha_{2}) \sum_{z1} \sum_{z2} p(z_{1} \lvert \pi_{1}) p(z_{2} \lvert \pi_{2}) p(x \lvert \theta_{z_{1}z_{2}}) d \pi_{1} d \pi_{2}$$

이 식에 따르면, row/column에 해당되는 mixing weight는 ($$\pi_{1} / \pi_{2}$$) 한번씩만 샘플링되기 때문에, 각 row/column에 안에 속해있는 entry(data) 끼리는 coupling이 일어 난다. (Not statistically independent).  
$$\pi_{1}, \pi_{2}$$가 모든 row/column에 대해 sampling 되었을 때, ($$\pi_{1u}, \pi_{2v}$$) 각 entry들은 서로 `conditionally independence`를 만족한다.

**전체 matrix의 joint probability**의 수식을 쓰게되면 다음과 같다.

$$ p(X, \pi_{1u}, \pi_{2v}, z_{1uv},z_{2uv},u_{1}^{n1},v_{1}^{n2} \lvert \alpha_{1},\alpha_{2},\Theta) = $$  
 $$ ( \Pi_{u} p(\pi_{1u} \lvert \alpha_{1}) ) ( \Pi_{v} p(\pi_{2v} \lvert \alpha_{2}) )
( \Pi_{u,v} p(z_{1uv} \lvert \pi_{1u}) p(z_{2uv} \lvert \pi_{2v}) p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}} )^{\delta_{uv}}    )$$

> $$\delta_{uv}$$: if $$x_{uv}$$ is missing then 0 otherise 1  (non-missing entry만 고려한다)  

**각 entry에 대해 `conditionally independence`라 가정하면 수식은 다음과 같다**

$$ p(X, \pi_{1u}, \pi_{2v}, z_{1uv},z_{2uv},u_{1}^{n1},v_{1}^{n2} \lvert \alpha_{1},\alpha_{2},\Theta) = $$  
 $$ ( \Pi_{u} p(\pi_{1u} \lvert \alpha_{1}) ) ( \Pi_{v} p(\pi_{2v} \lvert \alpha_{2}) )
( p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}} )^{\delta_{uv}} )$$

> $$ p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}} ) = \sum_{z_{1uv}} \sum_{z_{2uv}} p(z_{1uv} \lvert \pi_{1u}) p(z_{2uv} \lvert \pi_{2v}) p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}})$$


**위의 식을 $$\pi_{1u},\pi_{2v}$$에 대해 marginlizing하여 Matrix X에 대해 확률을 다시 쓰게 되면 다음과 같다.**

$$ p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) = $$  
$$ \int \int (\Pi_{u} p(\pi_{1u} \lvert \alpha_{1}) ) (\Pi_{v} p(\pi_{2v} \lvert \alpha_{2}) )
\Pi_{u,v} \sum_{z_{1uv}} \sum_{z_{2uv}} $$  
$$p(z_{1uv} \lvert \pi_{1u}) p(z_{2uv} \lvert \pi_{2v}) p(x_{uv} \lvert \theta_{z_{1uv},z_{2uv}})^{\delta_{uv}} d \pi_{11} d \pi_{12} ... d \pi_{1n_{1}} d \pi_{21} d \pi_{22} ... d \pi_{2n_{2}} $$

<br/>

---


# 4. Inference and Learning

- BCC model의 learning task는 을 likelihood of observing matrix X을 최대화 하는 $$\alpha_{1}, \alpha_{2}, \Theta$$ 값을 추론 하는 것이다.

- $$ p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) $$ 은 Intractable 하기에 이 함수의 log-likelihood의 lower bound 함수를 정의한다.

- lower bound를 최대화 하는 model parameter ($$\alpha_{1}, \alpha_{2}, \Theta$$)를 찾는다
- lower bound를 찾기 위해, entire family of parameterized lower bounds와 이에 대한 set of free variational parameters를 고려할 것이다.


## **4.1 Variational Approximation**
- [Variational Approximation 정리한 글](https://swha0105.github.io/ml_dl/2021/07/13/ML_DL-varience_inference/)
- $$ p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) $$의 lower bound를 찾기 위해 **latent variable distribution을 ($$p(z_{1},z_{2},\pi_{1},\pi_{2} \lvert \alpha_{1}, \alpha_{2}, \Theta)$$)** 근사하는 아래와 같은 함수(q) 를 도입한다. 



|![BCC model](https://swha0105.github.io/assets/ml/img/BCC_fig2.png)  
|:--:| 
| Variational distribution q |  


$$q(z_{1},z_{2},\pi_{1},\pi_{2} \lvert \gamma_{1}, \gamma_{2}, \phi_{1},\phi_{2}) = $$   
$$\Pi_{u=1}^{n_{1}} q(\pi_{1u} \lvert \gamma_{1u}) \Pi_{v=1}^{n_{2}} q(\pi_{2v} \lvert \gamma_{2v}) \Pi_{u=1}^{n_{1}} \Pi_{v=1}^{n_{2}} q(z_{1uv} \lvert \phi_{1u})  q(z_{2uv} \lvert \phi_{2v}) $$
 
> $$\gamma_{1} = (\gamma_{1u}, u_{1}^{n_{1}} )$$: Dirichlet distribution parameter  
> $$\phi_{1} = (\phi_{1u}, u_{1}^{n_{1}})$$: Discrete distribution parameter  
> $$m_{u}$$ (in figure) = number of non-missing entries in row u  

- 기존 LDA나 BNB에서 쓰는 variational approximation의 방법은 다음과 같다. 각 entry의 할당된 cluster assignment(z)는 각기 다른 variational discrete distribution ($$\phi$$) 생성한다. 
- BCC는 이에 다르게 하나의 row/column에 대해 같은 variational discrete distribution을 사용한다.

- 이와 같은 방법을 통해 다음과 같은 2가지의 장점을 얻을 수 있다.
  1. row/column 안에 있는 entries들에 대해 dependency를 유지할 수 있다. 
  2. number of variational parameter가 줄기 때문에 inference가 빨라 진다.

<br/>

- 전체 Matrix probability 에 대한 식은 다음과 같다.

$$ \log p(X \lvert \alpha_{1}, \alpha_{2}, \Theta) \geq E_{q} (\log p(X,z_{1},z_{2},\pi_{1},\pi_{2} \lvert \alpha_{1}, \alpha_{2}, \Theta))$$   
$$- E_{q} (\log q(X,z_{1},z_{2},\pi_{1},\pi_{2} \lvert \gamma_{1}, \gamma_{2}, \phi_{1},\phi_{2})) $$

- 이 식의 `Lower bound`다음과 같지만 이것을 유도하거나 정리하는건 복잡하고 의미 없는거같아 논문에서도 언급만 하고 넘어 간듯하다. 

$$L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}; \alpha_{1}, \alpha_{2},\Theta)  = E_{q}( \log p(\pi_{1} \lvert \alpha_{1})) + E_{q}( \log p(\pi_{2} \lvert \alpha_{2})) $$
$$ + E_{q}( \log p(z_{1} \lvert \pi_{1})) + E_{q}( \log p(z_{2} \lvert \pi_{2})) + E_{q}( \log p(X \lvert z_{1}, z_{2},\Theta))$$ $$- E_{q}( \log p(\pi_{1} \lvert \gamma_{1})) -  E_{q}( \log p(\pi_{2} \lvert \gamma_{2})) - E_{q}( \log p(z_{1} \lvert \phi_{1})) -  E_{q}( \log p(z_{2} \lvert \phi_{2})) $$



### **4.1.1 Inference**

- Inference step에서는 $$ \log p(X \lvert \alpha_{1},\alpha_{2},\Theta)$$의 **lower bound (L)의 parameter $$(\alpha_{1},\alpha_{2},\Theta)$$를 설정하여 Maximize한다.**

- Lower bound의 closed form solution이 없기 때문에 Lower bound의 maximum을 구하기 위해 $$ \frac{\partial L}{\partial \phi} = 0, \; \frac{\partial L}{\partial \gamma} = 0$$ 인 $$\phi, \gamma$$을 구한다.

$$ \phi_{1ui} \propto \exp(\Psi_{1ui}) + \frac{\sum_{v,j}} \delta_{uv} \phi_{2vj} \log p(x_{uv} \lvert \theta_{ij}){m_{u}}$$  
$$\gamma_{ui} = \alpha_{1i} + m_{u}\phi_{1ui}$$

> $$\phi_{1ui}$$: row u에 대한 $$\phi_{1}$$의 i번째 component  
> $$\phi_{2vj}$$: column v에 대한 $$\phi_{2}$$의 j번째 component  
> $$\Psi$$: digamma function
> i,j: co-cluster의 index  


- $$\phi_{1ui}$$는 row u 가 cluster i 에 대해 속할 수 있을 정도  
(degree of row u belonging to cluster i)

- $$\phi$$와 $$\gamma$$를 추론하기 위해 [Simulated Annealing](https://youtu.be/qK46ET1xk2A?t=1785)을 사용하였다. 논문에 자세한 설명이 나오지 않아 링크를 참조하였다.


### **4.1.2 Parameter Estimation**

- Inference에서 $$\alpha_{1},\alpha_{2},\Theta$$을 이용하여 최적의 **($$\gamma_{1}^{*}, \gamma_{2}^{*}, \phi_{1}^{*}, \phi_{2}^{*}$$)** 을 구하였다. $$L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}; \alpha_{1}, \alpha_{2},\Theta) $$ 

- 최적의 $$\alpha$$을 구하기 위해 LDA, BNB에서 사용한 `Newton method`를 사용한다. 

$$ \alpha_{1}^{'} = \alpha_{1} + \eta H(\alpha_{1})^{-1} g(\alpha_{1})$$

- $$\alpha$$가 feasible region ($$\alpha$$ >0)에서 벗어나는걸 방지하기위해 $$\eta$$값을 조절하는 `adaptive line search`를 적용.

- $$\Theta = (\mu_{ij}, \sigma_{ij}^{2},[i]_{1}^{k1},[j]_{1}^{k2})$$는 모든 exponential family distribution의 solution이 될 수 있지만 여기서는 univariate Gaussians이라 가정한다.

$$u_{ij} = \frac{ \sum_{u=1}^{n_{1}} \sum_{v=1}^{n_{2}} \delta_{uv}x_{uv}\phi_{1ui}\phi_{2vj} }{\sum_{u=1}^{n_{1}} \sum_{v=1}^{n_{2}} \delta_{uv}\phi_{1ui}\phi_{2vj}}$$


### **4.2 EM algorithm**

- **지금까지 `intractable posterior probability` ($$\log p(X \lvert \alpha_{1},\alpha_{2},\Theta)$$)을 추론하기 위해 `Variational Approximation` ($$q(z_{1},z_{2},\pi_{1},\pi_{2} \lvert \gamma_{1},\gamma_{2},\phi_{1},\phi_{2})$$을 구성한 뒤 posterior와 차이를 최소화 하는 `Lower bound`($$L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2};\alpha_{1},\alpha_{2},\Theta)$$)을 구성했다. 이 후, `Inference`와 simulated annealing을 통해 Lower bound를 최대화 하는 variational parameters ($$\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}$$)을 계산하고 `Parameter Estimation`와 newton method 통해 최적의 Dirichlet parameters ($$\alpha_{1},\alpha_{2}$$)을 계산했다.**

**1. E-step:** given ($$\alpha_{1}^{t-1},\alpha_{2}^{t-1},\Theta^{t-1}$$)  
- $$\gamma_{1}^{t},\gamma_{2}^{t},\phi_{1}^{t},\phi_{2}^{t} = argmax_{\gamma_{1},\gamma_{2},\phi_{1},\phi_{2}} L(\gamma_{1},\gamma_{2},\phi_{1},\phi_{2};\alpha_{1}^{t-1},\alpha_{2}^{t-1},\Theta^{t-1})$$

- 이 과정을 통해 $$\log p(X \lvert \alpha_{1},\alpha_{2},\Theta)$$의 Lower bound function (L)을 구할 수 있다.

**2. M-step:** 

- $$(\alpha_{1}^{t},\alpha_{2}^{t},\Theta^{t}) = argmax_{\alpha_{1},\alpha_{2},\Theta} L(\gamma_{1}^{t},\gamma_{2}^{t},\phi_{1}^{t},\phi_{2}^{t};\alpha_{1},\alpha_{2},\Theta)$$

- 이 과정을 통해 현재 (t)의 lower bound의 최적의 dirichlet parameter를 구할 수 있다. 


<br/>

----

# 5. Experiments

- Simulated data와 Real data(스킵)에 대해 테스트 하였다.

## 5.1 Simulated data

- 20x20의 Co-cluster(subcluster, 4 row, 5 column)으로 구성된 3개의 Data matrix (80x100)을 이용한다. 각 data matrix에 대해 generative model은 Gaussian, Bernoulli, Possion이다.

- co-cluster의 data는 generative model with pre-defined parameter 에서 생성되고 각 co-cluster의 paramter는 각기 다르다.

- 생성된 data들은 row, column단위로 randomly permute된다. permute된 데이터를 이용해 permute되기 전 데이터에 대한 정보를 추론한다(label).

- 각각의 Co-cluster의 대해 5%의 label을 주어 **semi-supervised learning**을 한다.

- Cluster accuracy(CA)(= $$\frac{1}{n} \sum_{i=1}^{k} n c_{i} $$)를 계산한다
> n: number of rows/columns  
> k: number of row/column clusters  
> $$c_{i}$$: i번재 co-cluster에 대하여, 가장 많은 수의 row/column의 parameter들이 same true cluster의 parameter와 일치하는지 알려주는 변수. ~~논문에 설명이 부족하여 추론한 내용, 틀릴 수 있음~~


|![parameter estimation](https://swha0105.github.io/assets/ml/img/BCC_fig3.png)
|:--:| 
| Top: parameter estimation for Gaussian, Top left: True, Top Right: Estimated |  
| Bottom: cluster accuracy |  

<!-- |![Cluster accuracy](https://swha0105.github.io/assets/ml/img/BCC_table.png) 
|:--:| 
| cluster accuracy |   -->


- Parameter estimation에 대해, 각 co-cluster에 대해 highest log-likelihood를 표현하는 parameter를 pick했다.
- **BCC는 generative model을 다르게하여 다양한 data type에 적용할 수 있다.**

<br/>

## 5.2 Real Data
- `Movieles`, `Jester`, `Foodmart` 총 3개의 데이터셋으로 테스트 하였다.
- 3개의 데이터에 대해 **Binarzied**한 버전과 original 버전을 각각 테스트 하였다.

1. Movieles
  - Movie 추천 데이터로 1~5점으로 구성된 100,000 rating, 1682개의 movie, 943명의 user로 구성되어있다.
  - 3점 이상은 1, 그 이하는 0으로 **Binarized**를 하였고 이에 대해 **Bernoulli distribution**을 사용하였다
  - Original dataset에는 **Discrete distribution**을 Generative model로 사용하였다

2. Jester
  - Joke rating 데이터셋으로 -10~+10점으로 구성된 (4.1백만개의 연속적인 rating이 존재) 100개의 Joke와 73,421명의 유저가 존재한다
  - 100개의 joke에 대해 모두 rating을 한 1000명의 유저를 선택하여 dense matrix를 구성하였다
  - 0~+10 (Non-negative)의 rating을 1, 그 외의 rating들을 0으로 하여 **Binarized**를 수행하였고 이에 대해 **Bernoulli distribution**을 사용하였다
  - Original dataset에는 **Poission distribution**을 Generative model로 사용하였다

3. Foodmart
  - 가상의 유통거래 데이터셋으로 164,558개의 sales record와 7803명의 고객, 1559개의 상품이 있다.
  - 각각의 record는 고객이 상품을 산 데이터를 의미한다.
  - product의 갯수가 median보다 낮으면 0, 높으면 1로 **Binarized**을 하였고 이에 대해 **Bernoulli distribution**을 사용하였다
  - row, column에 10개의 missing data가 있으면 데이터를 삭제하엿다.
  - Original dataset에는 **Gaussian distribution**을 Generative model로 사용하였다


### 5.2.1 Moethodology

- 각각의 모델들은 **Section 4**에서 언급한 EM algorithm을 통해 학습된다. 
- 모델의 prediction perfoemance를 evaluation하는 방법은 다음과 같다

  **1.** Traning과 Test data를($$X_{train}, X_{test}$$)을 이용해 E-step에서 variational parameter($$ \gamma_{1},\gamma_{2},\phi_{1},\phi_{2})$$)를 평가한다. 

  **2.** variational parameter와 model parameter($$\alpha_{1}, \alpha_{2}, \Theta$$)을 이용하여 **Perplexity** ($$Perp(X_{test}$$))를 계산한다.

    > Perplexity = $$ e^{- \log p(X)/ N} $$ 
    > N: Non-missing entries
    > X: dataset
    > - low perplexity는 test set에 대한 높은 log-likelihood를 가지고 있다. 


  **3.** test데이터 셋에 noise를 첨가하여 noisy data를 생성한다 ($$\tilde{x}$$). 이후, $$X_{train},\tilde{X_{test}}$$를 이용해 ($$ \tilde{\gamma_{1}},\tilde{\gamma_{2}},\tilde{\phi_{1}},\tilde{\phi_{2}})$$)을 계산한다.

- 만약, $$Perp(X_{test}) \lt Perp(\tilde{X_{test}})$$ 라고 하면 모델이 noisy data보다 원래 데이터셋에 대해 더 좋은 성능을 보인다고 해석이 가능하다. 

- 만약 좋은 모델이라면, noise가 심해질수록 $$Perp(\tilde{X})$$가 증가할 것이다. (노이즈 데이터에 대해 성능이 안좋아짐.)

<br/>


**Perplexity Comparsion**
 
|![Perplexity for binarized Jester estimation](https://swha0105.github.io/assets/ml/img/BCC_fig4.png)
|:--:| 
| Fig 4. Perplexity for binarized Jester estimation |  

|![Perplexity for original Movielens estimation](https://swha0105.github.io/assets/ml/img/BCC_fig5.png)
|:--:| 
| Fig 5. Perplexity for original Movielens estimation |  
  > LDA는 Binarized data에만 사용이 가능하여 fig 5에는 생략되어있다.  
  > 여기서 row clsuter는 user or customer cluster, column cluster는 movie, jove, product를 의미한다.

|![table for comparison](https://swha0105.github.io/assets/ml/img/BCC_table3.png)
|:--:| 
| talbe 3. table for comparison |  
  > row cluster를 10개로 고정한 셋팅



- Perplexity 측면에서, BCC는 LDA에 비해 2~3 order of magnitude정도 낮다
- 하지만 BCC와 LDA는 서로 다른 데이터 타입에 사용되도록 설계되었기에 해석하는데 주의를 요구한다.

- Perplexity 측면에서, BNB는 BCC에 비해 training set은 좋은 성능을 보이지만 test set에 대해서는 떨어지는 성능을 보인다

- figure 5에서 clusster가 늘어날때, perplexity가 급격히 증가하는건 overfitting을 의미한다
  - BCC가 overfitting을 피해간 이유로 2가지 추측을 할 수 있는데  variational paramter가 BNB에 많다는 것과 BNB가 캡쳐하지 못한 co-cluster structure를 캡쳐할 수 있는 능력으로 추측할 수 있다.

<br/>

**Prediction Comparsion**

- **Prediction evaluation을 위해 noisy data 사용** ($$X_{train}, \tilde{X_{test}}$$)
- real recommender system에서, 유저가 like or dislike하는 데이터를 묘사하기 위해 Binarized data에서만 비교하였다.

|![Perplexity curves for noise](https://swha0105.github.io/assets/ml/img/BCC_fig6.png)
|:--:| 
| Fig 6. BCC model for Perplexity curves along with noisy |  

|![Perplexity curves for binarized Jester](https://swha0105.github.io/assets/ml/img/BCC_fig7.png)
|:--:| 
| Fig 7. BCC vs LDA for Perplexity for binarized Jester | 

|![Perplexity curves for binarized Movielens](https://swha0105.github.io/assets/ml/img/BCC_fig8.png)
|:--:| 
| Fig 8. BCC vs LDA for Perplexity for binarized Movielens | 

- Figure 6에서, BCC모델은 noise가 증가할수록 Perplexity가 증가함을 보였다. 이는 noise를 잘 detect함을 의미하고 좋은 모델임을 반증하는것이다.

- Figure 7에서, LDA모델은 noise가 증가함에 따라 perplexity가 monotonically 증가함이 아니라, 감소하는 경향도 보이는데 이는 noise와 실제데이터를 구분을 못함을 의미한다.
 
- BCC는 non-missing data에서, LDA는 entry value가 1인 data에서만 수행하였기 때문에 직접적인 비교는 주의해야한다. 하지만 전반적인 알고리즘의 행동(Perplexity range, number of clusters and trends)를 비교하는데 참고할만 하다. 


<br/>

**Visualization**

|![Co-cluster paramters for Movielens](https://swha0105.github.io/assets/ml/img/BCC_fig9.png)
|:--:| 
| Fig 9. Co-cluster paramters for Movielens | 

- 10개의 user-cluster와 20개의 moview cluster로 이루어진 데이터.
- sub-block의 명암은 bernoulli distribution의 parameter value를 의미한다. 진할수록 user cluster가 movie cluster를 선호함을 의미한다.

- U2(위에서 두번째 row)는 거의 모든 movie를 좋아한다, U5는 거의 모든 movie를 싫어한다 (except M13 (column cluster 13))

- BCC는 row / column에 대해 simultaneous하게 dimension reduction을 줄일 수 있고 이 dimension은 variational parameter ($$\phi_{1}$$, $$\phi_{2}) (dimension: k1,k2)로 표현된다. (**low-dimensional representation**)

- low-dimensenal vector인 $$\phi_{1}, \phi_{2}$$을 **co-embedding**이라 부른다.


---

# 6. Conclusion

- BNN은 Co-clustering을 generative mixture modeling problem의 시각으로 풀어냈다.
- sparse matriex에 강하며 generative model로 any exponential family을 사용할 수 있다.
- rows/columns에 대해 co-cluster을 하나만 배정하는 partitional co-clustering과는 다르게 rows and column에 대해 mixed membership을 생성한다.
- 제시한 variational approximation은 stochastic approximation에 비해 significantly faster한 강점을 가지고 있다.



--

<script>
MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
