---
layout: post
title:  "[ML/DL Lecture] Support Vector Machine 1 "
subtitle:   "Machine learning"
categories: ml_dl
tags: ml
comments: true
---

## 들어가기 전
항상 강의듣기 전에는 이 내용을 다 알고 있다고 생각한다. SVM도 관련해서 공부해본적 있어서 이걸 굳이 들어야하나.. 의문을 가지고 강의를 들었다.  
그리고 역시 부족한부분이 있었고 강의 듣길 잘했다고 생각한다.. 항상 이런일이 반복되는데 배움앞에서 겸손하자  
[김성범 교수님 강의](https://www.youtube.com/watch?v=qFg8cDnqYCI&list=PLpIPLT0Pf7IoTxTCi2MEQ94MZnHaxrP0j&index=12)를 참조하여 정리 하였다. 

<br/>

- Support Vector Machine
- Margin
- Convex Optimazation Problem
- Solution by Karush-Kuhn-Tucker(KKT) condition
- lagrangian primal & dual

---

# Support Vector Machine 

|![BF](https://swha0105.github.io/assets/ml/img/svm_hpyerplane.png)   
|:--:| 
| SVM의 Hyperplane |
 
- SVM은 high-dimensional data의 `classification 문제`를 **`Quadratic Programming(QP)`**로 변형하여 푼다.
- SVM은 statisical learning theory를 기반한 모델이다.
- SVM은 traning error를 줄이는 방향으로 학습하면 testing 에러가 줄어든다 (Generalization ability)
  
   
결국, **`Hyperplane의`** `w` (법선벡터) 와 `b` (bias)을 데이터에 맞게 설정(학습)하는게 SVM의 목적이다.


<br/>

---

# Margin

그렇다면 좋은 SVM은 무엇을 의미할까?. 직관적으로 생각해보면 2개의 클래스를 가운데로 나누는 `hyperplane`을 찾는 것이라고 생각할 수 있다.  


|![BF](https://swha0105.github.io/assets/ml/img/svm_margin.png)   
|:--:| 
| Margin |

- Margin: 각 클래스에서 서로 가장 가까운 관측치 사이의 거리, `W`(법선벡터)로 표현가능.
- Support Vector: 각 클래스에서 서로 가장 가까운 데이터 포인트.  
위의 그림에서 $$w^{T} x + b = \pm 1 $$  

**이때, 좋은 SVM이란 traning set의 margin을 최대화 하는것이다.**    

Class 2에 있는 $$x^{+}$$와 Class 2의 $$x^{-}$$의 상관관계는 **평행이동** 관계라고 볼 수 있다.    
이 성질을 이용해 평행이동 정도인 $$\lambda$$ 을 구해보자. ($$x^{+} = x^{-} + \lambda w$$)
  
각각의 **support vector를 연립**하면,  
$$w^{T} x^{+} + b = 1$$ $$\; w^{T} (x^{-} + \lambda w) + b = 1 $$  
$$w^{T} x^{-} + b + \lambda w^{T}w = 1 \quad (w^{T}x^{-} + b = -1) $$  
$$ \lambda = \frac{2}{w^{T}w}$$  가 된다.  

<br/>

이 $$\lambda$$ 를 이용해 **Margin을 수학적으로 정의**해보자.    
Margin은 그 정의에 의해 $$ \rvert \rvert x^{+} - x^{-} \rvert \rvert_{2}$$ 로 표현 할 수 있다. 
$$ = \; \rvert \rvert (x^{-} + \lambda w)  - x^{-} \rvert \rvert_{2} \; = \; \rvert \rvert \lambda w \rvert \rvert_{2} \; = \; \lambda \sqrt{w^{T}{w}} \;$$  (by L2-norm definition)  
$$ \frac{2}{w^{T}w} \sqrt{w^{T}w}  \; = \; \frac{2}{\sqrt(w^{T}w)} \; = \; \frac{2}{\rvert \rvert w \rvert \rvert_{2}}$$
  
> n1-norm: $$ \rvert \rvert w \rvert \rvert_{1} = (\sum_{i} \rvert w_{i} \rvert^{1})^{\frac{1}{1}} $$  
> n2-norm: $$ \rvert \rvert w \rvert \rvert_{2} = (\sum_{i} \rvert w_{i} \rvert^{2})^{\frac{1}{2}} = \sqrt{w_{1}^2 + w_{2}^2 + ... w_{n}^2} = \sqrt{W^{T}W}$$



따라서, **좋은 SVM이란, Margin (= $$\frac{2}{\rvert \rvert w \rvert \rvert_{2}}$$)값을 최대화 하는것이다.**

<br/>

---

# Convex Optimazation Problem
그렇다면, Margin을 최대화 할 때, `W`,`b`값은 어떻게 구할 수 있을까?  

문제를 정리하자면 **SVM은 margin을 최대화 해야하고, 이 말은 다음과 같다.**


$$ min \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} \quad \; s.t \quad y_{i}(w^{T}x_{i} +b ) \geq 1, \; \forall i \quad \;  (\forall = for \; all)  $$
  
> - $$\frac{2}{\rvert \rvert w \rvert \rvert_{2}}$$의 최댓값을 구하는 것은 $$ \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} $$ 의 최소값을 구하는 것과 같다.  
> - $$\rvert \rvert w \rvert \rvert_{2}$$에 제곱이 붙은건 계산의 편의성 때문  
> - subject to로 시작하는 `제약식`가 의미하는 바는, 모든 데이터는 support vector의 테두리 안에 있어야 된다는 의미
  

위와  같이 제약식이 있는 상황에서 min,max를 구하는 문제는 `lagrangian multipier method`를 통해 풀 수 있다. (Optimization with inequality constraints)  
이를 통해 위의 식을 정리하면 다음과 같다.

$$ \underset{w,b}{min} L(w,b,\lambda) = \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} - \sum_{i=1}^{n} \lambda_{i}(y_{i}(w^{T}x_{i} + b) -1 )  $$  

위의 식을 풀기 위해 w,b에 대해 편미분을 하여 0이 되는 지점을 찾게 되면 (2차 식이기 때문에 0이 되는 지점에서 최솟값)  

$$ \frac{\partial L(w,b,\lambda)}{\partial w} = 0 \quad  where \quad w = \sum_{i=1}^{n} \lambda_{i}y_{i}x_{i} $$  
  
$$ \frac{\partial L(w,b,\lambda)}{\partial b} = 0 \quad  where \quad b = \sum_{i=1}^{n} \lambda_{i}y_{i}x_{i} $$  
  
L이 최소가 되는 w,b 지점을 찾았으니 w,b값을 대입하여 **L의 최솟값**을 다시 구해보자. (유도과정 생략)  

$$ \underset{w,b}{min} L(w,b,\lambda) = \underset{w,b}{min} \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} - \sum_{i=1}^{n} \lambda_{i}(y_{i}(w^{T}x_{i} + b) -1 )  $$   
  
$$ = \sum_{i=1}^{n}\lambda_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j} x_{i}^{T} x_{j}$$  
  
  
이때, $$\underset{w,b}{min} L(w,b,\lambda)$$ 을 구하는 것은 $$ \sum_{i=1}^{n}\lambda_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_{i} \lambda_{j} y_{i} y_{j} x_{i}^{T} x_{j}$$의 **최댓값을 구하는 것과 같다.** **(`Lagrangian dual`)**  

<br/>

---

# Solution by Karush-Kuhn-Tucker(KKT) condition
위에서 구한 **(`Lagrangian dual`)** 문제를 구하는 솔루션은 Lagrangian이 `KKT 조건`을 만족하는 것과 같다.  
자세한건 강의에서 설명하지 않고 찾아보니 너무 수학적이라 생략한다.  

$$ L(w,b,\lambda) = \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} - \sum_{i=1}^{n} \lambda_{i}(y_{i}(w^{T}x_{i} + b) -1 ) $$ 에서,   

`KKT condition`중 몇 가지 성질만 보게되면  


**Stationarity** : $$ w = \sum_{i=1}^{n}\lambda_{i}y_{i}x_{i} \quad (\frac{\partial L(w,b,\lambda)}{\partial w} = 0) $$


**Complementary slackness**: $$\lambda_{i} (y_{i} ( w^{T}x_{i} + b ) -1 ) = 0$$

**Complementary slackness**을 만족하는 2가지 케이스에 대해 조사하면, 
1. $$\lambda_{i} \gt 0 $$ 이면  
$$ y_{i} ( w^{T}x_{i} + b ) = 1 $$ 을 만족 해야한다.    
`support vector`는 **각 클래스의 plane 위에 있으므로** $$y_{i}(w^{T}x_{i} + b) = 1$$ 을 만족한다. 따라서 위의 식을 만족하는 데이터 포인트는 `support vector`이다.

2.  $$\lambda_{i} = 0 $$ 이면  
$$ y_{i} ( w^{T}x_{i} + b ) \ne 1 $$ 이다.  
따라서, **각 클래스 plane 안에 있는 데이터 포인터들을** 의미한다

<br/>

돌고 돌아, 결국 우리의 목적인 `w`를 구해보자.   

KKT의 조건 중, **Stationarity**의 수식을 보게되면,  
$$ w = \sum_{i=1}{n}\lambda_{i}y_{i}x_{i} \quad (\frac{\partial L(w,b,\lambda)}{\partial w} = 0) $$ 에서, $$\lambda \gt 0 $$ 일때만 의미를 갖고 이 조건을 만족하는 데이터 포인트는 **`support vector`** 이다. 따라서 **`support vector`**만 이용하여 **`hyper plane`**을 구할 수 있다.  
  

<br/>

---

## 찾아본 내용 (lagrangian primal & dual)
$$ f^{*} \geq \underset{w,b \subset C}{min} L(w,b,\lambda) \geq  \underset{w,b}{min} L(w,b,\lambda) := g(\lambda)$$ 

`lagrangian primal` 이란, lagrangian을 통해 최적의 값 ($$f^{*}$$)을 찾고자 할때 성립하는 수식이다. 
위의 식을 잘 살펴보면, $$ \underset{w,b \subset C}{min} L(w,b,\lambda) \geq  \underset{w,b}{min} L(w,b,\lambda) $$ 은 ~~trivial~~ w,b에 대해 왼쪽식이 오른쪽 식 보다 범위가 줄어들기 때문에 당연히 성립한다.  

이때 $$g(\lambda) := \underset{w,b}{min} L(w,b,\lambda) $$은 `lagrangian dual function`이라 정의한다.  
`Lagrangian primal problem`은 우리가 찾고자 하는 $$f^{*}$$ 에 **`lower bound`**를 제공해주데 의미가 있다. 
  
하지만 `lower bound`를 제공하는데 만족하지 못하고 직접 값을 구해야 한다면 어떻게 해야할까?  
위의 수식에서 $$f^{*}$$을 찾는 문제는 $$g(\lambda)$$를 **최대화 하는 문제와** 같게 된다.   
따라서, $$g(\lambda)$$를 구해 최대화 하는 문제로 변형한 뒤 풀면 된다. 
이와 같은 과정을 **`lagrangian primal`**문제를 **`largrangian dual`** 로 변형시켜 푼다 라고 한다.  
(보통의 경우 dual이 primal보다 풀기 쉬움.)

<br/>

### ref:
1. https://ratsgo.github.io/convex%20optimization/2018/01/25/duality/  
2. https://psystat.tistory.com/103  


<script>
MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
