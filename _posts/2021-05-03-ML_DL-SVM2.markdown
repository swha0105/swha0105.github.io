---
layout: post
title:  "[ML/DL Lecture] Support Vector Machine 2"
subtitle:   "Machine learning"
categories: ml_dl
tags: ml
comments: true
---

[김성범 교수님 강의](https://www.youtube.com/watch?v=ltjhyLkHMls)를 참조하여 정리 하였다.  
[앞선 강의](https://swha0105.github.io/ml_dl/2021/04/15/ML_DL-SVM1/) 와 이어지는 내용이다.

- Linearly Nonseparable Case (Soft Margin SVM)
- Linearly Nonseparable Case (Kernel method)

<br>

---

# Soft Margin SVM 


|![soft margin svm](https://swha0105.github.io/assets/ml/img/soft_svm.png)   
|:--:| 
| Soft Margin svm |

$$ \underset{w,b}{min} L(w,b,\xi) \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} + c \sum_{i}^{n} \xi_{i}$$ 

- 위 그림과 같이 Linearly Nonseparable Case 일때, 에러 $$\xi$$를 허용해준다.
- c $$\sum_{i}^{n} \xi_{i}$$은 Regularization 효과와 같음. 즉, Overffiting 방지 효과 
- c 는 margin과 traning-error에 대한 trade-off를 결정하는 `hyperparameter`
  - c가 증가 -> traning error 감소 -> overfit ($$\xi$$ 에 대한 제약 증가. 에러 허용 안함)
  - c가 감소 -> traning error 증가 -> underfit

|![soft margin](https://swha0105.github.io/assets/ml/img/soft_margin.png)   
|:--:| 
| Soft Margin |

<br/>

### Lagrangian Formulation

$$ min \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} + c \sum_{i}^{n} \xi_{i} $$   
$$subject \; to \quad y_{i}(w^{T}x_{i} + b ) \geq 1 -\xi_{i} \quad (y \subset {1,2,3,...n}) $$   
의 식을 `Lagrangian Formulation`으로 바꾸면 다음과 같다.

$$ \underset{\alpha,\gamma}{max} \; \underset{w,b,\xi}{min} L(w,b,\alpha,\xi,\gamma) = \frac{1}{2} \rvert \rvert w \rvert \rvert_{2}^{2} - \sum_{i=1}^{n} \alpha_{i}(y_{i}(w^{T}x_{i}+b) - 1 + \xi_{i}) - c \sum_{i=1}^{n} \gamma_{i} \xi_{i}$$  
$$subject \; to \quad \alpha_{i}\gamma_{i} \geq 0 $$ 

위의 식에서 각 변수에 대해 편미분을 취하면 다음과 같다

$$ \frac{\partial L(w,b,\alpha,\xi,\gamma)}{\partial w} = 0 \quad \quad w = \sum_{i}^{n} \alpha_{i}y_{i}x_{i}$$  
$$ \frac{\partial L(w,b,\alpha,\xi,\gamma)}{\partial b} = 0 \quad \quad \sum_{i}^{n} \alpha_{i}y_{i} = 0 $$  
$$ \frac{\partial L(w,b,\alpha,\xi,\gamma)}{\partial \xi_{i}} = 0 \quad \quad C-\alpha_{i} - \gamma_{i} = 0 $$  

유도된 위의 3개의 식을 이용하여 다시 `Lagrangian Formulation`을 정리하면 다음과 같다.

$$ \underset{\alpha}{max} (\sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j} ) $$
$$ where \quad\sum_{i=1}^{n}\alpha_{i}y_{i} = 0 \quad C-\alpha_{i} - \gamma_{i} = 0 \quad \alpha_{i} \geq 0 \quad \gamma_{i} \geq 0$$

<br/>

위의 식을 앞선 강의에서 나온 `KKT condition`의 `Complementary slackness`의 성질을 이용해 수식과 솔루션을 유도하면 다음과 같다.

$$ \alpha_{i} ( y_{i} (w^{T}x_{i} + b) - 1 + \xi_{i} ) = 0 \quad \gamma_{i}\xi_{i} = 0 \quad C - \alpha_{i} - \gamma_{i} = 0 $$  
  
1. $$ \alpha_{i} = 0 $$  
$$ \gamma_{i} = C, \quad \xi_{i} = 0 \quad y_{i}((w^{T} + b) - 1) \neq 0 $$   
이때, $$x_{i}$$가 plane 위에 있지 않다. (plane 안쪽)

2. $$ 0 < \alpha_{i} < C $$  
$$ \gamma_{i} > 0 \quad \xi_{i} = 0 \quad \gamma_{i} \xi_{i} = 0 \quad (y_{i}(w^{T} + b) -1) = 0 $$  
이때, $$x_{i}$$는 plane 위에 있다 (support vector)

3. $$ \alpha_{i} = C $$  
$$ \gamma_{i} = 0 \quad \xi_{i} > 0 \quad -\gamma_{i} \xi_{i} = \alpha_{i}(y_{i}(w^{T} + b) -1)  $$  
이때, $$x_{i}$$는 plane 사이 안에 있다 (support vector, 엄밀히 말해서 plane위에 없지만 support vector라고 칭한다.)

|![soft soltion](https://swha0105.github.io/assets/ml/img/soft_margin.png)   
|:--:| 
| Soft margin SVM solution|

<br/>

---

# Kernel method

|![kernel method](https://swha0105.github.io/assets/ml/img/kernel_method.png)   
|:--:| 
| kernel method|

- Input 데이터의 차원에서 linear seperation이 안되는 데이터 분포일때, 데이터를 Kernel을 통과 시켜 **데이터의 차원의 수를 늘림.**
- 이후, 차원의 수가 늘어난 `feature space`에서 SVM을 학습.

### Kernel mapping
예를 들어, $$ X = (x_{1},x_{2}), Y = (y_{1},y_{2})$$ 의 데이터에 다음과 같은 `kernel function`을 이용해 `mapping`을 시킨다고 해보자.  
Kernel function: $$\phi(X) = (x_{1}^2,x_{2}^{2}, \sqrt{x_{1}x_{2}}) $$  

차원이 늘어난 데이터 셋을 다시 SVM의 lagrangian formulation에 넣어보면 다음과 같다.  

$$ \underset{\alpha}{max} (\sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i}\alpha_{j}y_{i}y_{j} \phi(X_{i})^{T} \phi(X_{j}) ) $$

`Kernel mapping`을 하게 되면 위와 같은 직관적인 식이 되지만, 계산량이 엄청 늘어난다.  
Input 데이터를 kernel function을 통해 `mapping`하고, `내적`을 해야하는데 Input 데이터의 보통의 크기를 생각하면 **어마어마한 연산량이 필요로 하다.**

### kernel trick
kernel function이 Input 데이터에 대해 `Linear Transformation`로 표현이 가능하다고 가정하자. ($$ \phi(X) = Ax $$ )  
그렇다면, $$\phi(X_{i})^{T} \phi X_{j} = x_{i}^{T} A^{T} A x_{j} = K(x_{i},x_{j})$$ 로 표현이 가능하다.  
이때, $$K(x_{i},x_{j})$$은 **`대칭행렬`이고** **element들은 `scalar 값` 이다.**  

<br/>

위와 같은 성질들을 만족하는 function들은 모두 `kernel function`이 될 수 있고 자주쓰는 함수는 다음과 같다.  

1. linear: $$K(x_{1},x_{2}) = x_{1}^{T}x_{2}$$  
2. polynomial: $$K(x_{1},x_{2}) = (x_{1}^{T}x_{2}+c)^{d}, \quad c>0$$   
3. sigmoid: $$K(x_{1},x_{2}) = \tanh(a(x_{1}^{T}x_{2}) + b)$$  
4. gaussian: $$K(x_{1},x_{2}) = \exp(-\frac{\rvert \rvert x_{1} - x_{2} \rvert \rvert_{2}^{2}}{2 \sigma^2}), \quad \sigma \neq 0 $$


이 **`kernel function`**들은 input 데이터를 이용해 특정한 규칙으로 feature space를 생성하기에 mapping과 내적을 동시에 하는 효과를 볼 수 있다.  
따라서, 연산을 획기적으로 줄일 수 있다고 한다.

<br>

---

#### ref
1. [참조 블로그](https://ratsgo.github.io/machine%20learning/2017/05/30/SVM3/)

<script>
MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
</script>

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
